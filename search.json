[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Polars",
    "section": "",
    "text": "Preface\nWelcome to “Introduction to Polars.” This book emerged from my journey as a data science student who chose to explore Polars rather than pandas—the standard library taught in my course. When I approached my instructor about using this alternative technology, he supported my decision while honestly acknowledging that course materials wouldn’t cover my chosen path.\nAs I navigated through the course, I discovered a significant gap in beginner-friendly Polars resources for data science newcomers. While the official documentation proved valuable, it often assumed a level of familiarity that beginners might not possess. Nevertheless, through persistence and experimentation, I gained proficiency and successfully completed my coursework.\nThis book aims to bridge that gap by offering an accessible introduction to Polars for those new to data manipulation libraries. I’ve designed it especially for readers with limited prior experience in data science, incorporating the insights and solutions I discovered along my learning journey.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Author",
    "section": "",
    "text": "I’m a Data Science student at Brigham Young University-Idaho with expertise in Python programming, data analysis, database management, and cybersecurity fundamentals. My journey in data science combines technical skills with practical problem-solving abilities.\n\n GitHub  LinkedIn  Website",
    "crumbs": [
      "About the Author"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What is Polars\nPolars is a modern data manipulation library avalible for Python, R, NodeJs and Rust. It is designed as a high-performance alternative to pandas, especially for large datasets. It features syntax that’s both human-readable and similar to R’s data manipulation paradigms. Polars stands out for three main reasons:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-is-polars",
    "href": "intro.html#what-is-polars",
    "title": "Introduction",
    "section": "",
    "text": "Performance: Built in Rust, Polars delivers exceptional speed through parallel processing by default and a sophisticated query optimizer that analyzes and improves execution plans.\nMemory efficiency: Using a columnar memory format rather than row-based storage, Polars efficiently handles larger-than-memory datasets and performs operations with minimal memory overhead.\nLazy evaluation: Polars supports both eager and lazy execution modes. The lazy API builds optimized query plans before execution, similar to database query planners, resulting in more efficient data processing pipelines.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "Introduction",
    "section": "What you will learn",
    "text": "What you will learn",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#how-this-book-is-organized",
    "href": "intro.html#how-this-book-is-organized",
    "title": "Introduction",
    "section": "How this book is organized",
    "text": "How this book is organized",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Basic Installation\nPolars can be installed using pip:",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#basic-installation",
    "href": "installation.html#basic-installation",
    "title": "Installation",
    "section": "",
    "text": "pip install polars",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#optional-dependencies",
    "href": "installation.html#optional-dependencies",
    "title": "Installation",
    "section": "Optional Dependencies",
    "text": "Optional Dependencies\nPolars offers various optional dependencies for specific use cases, which are omitted to reduce the footprint of the library. Throught this guide I will mention when specific dependancts are required/used.\nTo install all optional dependencies:\npip install 'polars[all]'\n\n\n\n\n\n\nNote\n\n\n\nI recommend installing all optional dependencies due to convenience. And the fact that the relative footprint is still not excessive.\n\n\n\nInteroperability\nPolars offers the following dependencys for increased interoperability between different librarys.\n\npandas: allows conversion to and from pandas dataframes/series\nnumpy: allows conversion between numpy arrays\npyarrow:allows for data conversion between PyArrow tables and arrays\npydantic: allows for conversion from Pydantic models to polars\n\npip install 'polars[pandas, numpy, pyarrow, pydantic]' # remove the unused dependencies\n\n\nExcel\nPolars has a few options for different engines used to convert xlsx files to a format more readable by polars.\nThe different engines avalible are:\n\ncalamine\nopenpyxl\nxlsx2csv\n\n\n\n\n\n\n\nTip\n\n\n\nThere are some differences in the engines performance and behaviour to learn more see the official documentation.\n\n\nAdditionally Polars support one other optional dependency related to Excel: - xlsxwriter: which allows you to write to xlsx files\npip install 'polars[excel]' # if you want to install all Excel dependencies\npip install 'polars[calamine, openpyxl, xlsx2csv, xlsxwriter]' # if you want to pick and chose \n\n\nDatabase\n\n\nCloud\n\n\nOther I/O\n\n\nOther",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Dataframes and Series",
    "section": "",
    "text": "1.1 Data types\nPolars allows you to store data in a variety of formats called data types. These data types fall generally into the following categories:\nThe most common data types you will be working with are generally: Strings, signed and unsigned integers, floating point numbers or floats, decimals, dates or datetimes and booleans. For more information on each of these data types see Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Dataframes and Series</span>"
    ]
  },
  {
    "objectID": "chapter1.html#data-types",
    "href": "chapter1.html#data-types",
    "title": "1  Dataframes and Series",
    "section": "",
    "text": "Numeric: Signed integers, unsigned integers, floating point numbers, and decimals\nNested: Lists, structs, and arrays for handling complex data\nTemporal: Dates, datetimes, and times for working with time-based data\nMiscellaneous: Strings, binary data, Booleans, categoricals, enums, and objects",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Dataframes and Series</span>"
    ]
  },
  {
    "objectID": "chapter1.html#series",
    "href": "chapter1.html#series",
    "title": "1  Dataframes and Series",
    "section": "1.2 Series",
    "text": "1.2 Series\nThe two most common data structures in Polars are DataFrames and Series. Series are one-dimensional data structures where\nCreating a Series is straightforward with the following syntax:\npl.Series(name, values_list)\nWhere “name” is the label for your Series and “values_list” contains the data. Here’s a simple example:\n\nimport polars as pl\ns = pl.Series(\"example\", [1, 2, 3, 4, 5])\ns\n\n\nshape: (5,)\n\n\n\nexample\n\n\ni64\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nWhen you create a series Polars will infer the data type for the values you provide. So in the above example I gave it [1, 2, 3, 4, 5] and it set the datatype to Int64 if instead gave it [1, 2, 3, 4.0, 5] it would asume it is Float64.\n\ns2 = pl.Series(\"payment\", [132.50, 120, 116, 98.75 ,42])\ns2\n\n\nshape: (5,)\n\n\n\npayment\n\n\nf64\n\n\n\n\n132.5\n\n\n120.0\n\n\n116.0\n\n\n98.75\n\n\n42.0\n\n\n\n\n\n\n\ns3 = pl.Series(\"mixed\", [1, \"text\", True, 3.14], strict=False)\n# series.dytpe outputs a the data type of the series\nprint(f\"Mixed series type: {s3.dtype}\")\ns3\n\nMixed series type: String\n\n\n\nshape: (4,)\n\n\n\nmixed\n\n\nstr\n\n\n\n\n\"1\"\n\n\n\"text\"\n\n\n\"true\"\n\n\n\"3.14\"\n\n\n\n\n\n\nYou can set the data type of the series as well by using the dtype parameter. A example use case is when storing a id number the id number should be stored as a string not a int due to the fact that we we do not want to perform mathmatical operations on the identification number therefore it is best stored as a string.\n\n# strict=False allows automatic conversion from different data types\ns3 = pl.Series(\"id number\", [143823, 194203, 553420, 234325, 236532], dtype=pl.Utf8, strict=False)\ns3\n\n\nshape: (5,)\n\n\n\nid number\n\n\nstr\n\n\n\n\n\"143823\"\n\n\n\"194203\"\n\n\n\"553420\"\n\n\n\"234325\"\n\n\n\"236532\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Dataframes and Series</span>"
    ]
  },
  {
    "objectID": "chapter1.html#dataframes",
    "href": "chapter1.html#dataframes",
    "title": "1  Dataframes and Series",
    "section": "1.3 Dataframes",
    "text": "1.3 Dataframes\nDataFrames are tabular data structures (rows and columns) composed of multiple Series, with each column representing a single Series. The design of a dataframe is called schema. A schema is a mapping of column to the data types.\nDataframes are the workhorses of data analysis and what you’ll use most frequently.\nWith DataFrames, you can write powerful queries to filter, transform, aggregate, and reshape your data efficiently.\nDataFrames can be created in several ways:\n\nFrom a dictionary of sequences (lists, arrays)\nWith explicit schema specification\nFrom a sequence of (name, dtype) pairs\nFrom NumPy arrays\nFrom a list of lists (row-oriented data)\nBy converting pandas DataFrames\nBy importing existing tabular data from CSVs, JSON, SQL, Parquet files, etc.\n\nIn real-world environments, you’ll typically work with preexisting data, though understanding various creation methods is valuable. We’ll cover data import techniques later, but for now, here’s an example of a DataFrame created from a dictionary of lists:\n\n# Create a DataFrame from a dictionary of lists\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n    \"age\": [25, 30, 35, 40],\n    \"city\": [\"New York\", \"Boston\", \"Chicago\", \"Seattle\"],\n    \"salary\": [75000, 85000, 90000, 95000]\n})\n\ndf\n\n\nshape: (4, 4)\n\n\n\nname\nage\ncity\nsalary\n\n\nstr\ni64\nstr\ni64\n\n\n\n\n\"Alice\"\n25\n\"New York\"\n75000\n\n\n\"Bob\"\n30\n\"Boston\"\n85000\n\n\n\"Charlie\"\n35\n\"Chicago\"\n90000\n\n\n\"David\"\n40\n\"Seattle\"\n95000\n\n\n\n\n\n\nevery data frame has a shape. the shape is the number of rows and columns in a dataframe shape(rows,columns)\nthe shape for the above dataframe is:\n\nprint(df.shape)\n\n(4, 4)\n\n\nyou can view the schema of any dataframe with the following command\n\nprint(df.schema)\n\nSchema({'name': String, 'age': Int64, 'city': String, 'salary': Int64})\n\n\nWe see here that the schema is returned as a dictionary. In the above example the column name has the string datatype. Though you can view the data type already when displaying the dataframe.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Dataframes and Series</span>"
    ]
  },
  {
    "objectID": "chapter1.html#inspecting-dataframes",
    "href": "chapter1.html#inspecting-dataframes",
    "title": "1  Dataframes and Series",
    "section": "1.4 Inspecting Dataframes",
    "text": "1.4 Inspecting Dataframes\nIn polars there are a varity of ways to inspect a dataframe, all of which have different use cases. The ones that we will be covering right now are:\n\nhead\ntail\nglimpse\nsample\ndescribe\nslice\n\n\n1.4.1 head\nthe head functions allows you to view the first x rows of the dataframe. By default the number of rows it shows is 5, though you can specify the number of rows to view.\ndataframe.head(n)\nWhere n is the number of rows to return if you give it a negative number it will turn all rows except the last n rows.\n\nimport numpy as np\n\n# Create NumPy arrays for sandwich data\nsandwich_names = np.array(['BLT', 'Club', 'Tuna', 'Ham & Cheese', 'Veggie'])\nprices = np.array([8.99, 10.50, 7.50, 6.99, 6.50])\ncalories = np.array([550, 720, 480, 520, 320])\nvegetarian = np.array([False, False, False, False, True])\n\n# Create DataFrame from NumPy arrays\nsandwich_df = pl.DataFrame({\n    \"sandwich\": sandwich_names,\n    \"price\": prices,\n    \"calories\": calories,\n    \"vegetarian\": vegetarian\n})\n\n\nsandwich_df.head(3)\n\n\nshape: (3, 4)\n\n\n\nsandwich\nprice\ncalories\nvegetarian\n\n\nstr\nf64\ni64\nbool\n\n\n\n\n\"BLT\"\n8.99\n550\nfalse\n\n\n\"Club\"\n10.5\n720\nfalse\n\n\n\"Tuna\"\n7.5\n480\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBoth head and tail are useful for quick data exploration without loading the entire dataset.\n\n\n\n\n1.4.2 tail\nThe tail function is essentially the inverse of head. It allows you to view the last n rows of the dataframe. The default for tail is also five rows.\ndataframe.tail(n)\nWhere n is the number of rows to return if you give it a negative number it will turn all rows except the first n rows.\n\n# Import data from the sales.csv file into a Polars DataFrame\nsales_df = pl.read_csv(\"./data/sales.csv\")\n\n# Display the last 6 rows of the sales DataFrame\nsales_df.tail(6)\n\n\nshape: (6, 8)\n\n\n\ndate\nregion\nsales_rep\nproduct_category\nunits_sold\nrevenue\ncost\ncustomer_segment\n\n\nstr\nstr\nstr\nstr\ni64\nf64\nf64\nstr\n\n\n\n\n\"2023-01-22\"\n\"East\"\n\"Emma Wilson\"\n\"Furniture\"\n4\n3599.96\n2519.97\n\"SMB\"\n\n\n\"2023-01-23\"\n\"West\"\n\"Michael Brown\"\n\"Electronics\"\n11\n5499.89\n3299.93\n\"Enterprise\"\n\n\n\"2023-01-23\"\n\"North\"\n\"John Smith\"\n\"Office Supplies\"\n83\n1660.0\n996.0\n\"Consumer\"\n\n\n\"2023-01-24\"\n\"South\"\n\"Maria Garcia\"\n\"Software\"\n27\n2699.73\n809.92\n\"SMB\"\n\n\n\"2023-01-24\"\n\"East\"\n\"David Johnson\"\n\"Electronics\"\n6\n2999.94\n1799.96\n\"Enterprise\"\n\n\n\"2023-01-25\"\n\"West\"\n\"Lisa Wong\"\n\"Furniture\"\n8\n7199.92\n5039.94\n\"Consumer\"\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor more information on reading in external data see Appendix B\n\n\n\n\n1.4.3 glimpse\nThe glimpse function allows you to preview your dataframe. By providing the number of rows and columns. The column names and datatypes and the first few values of each column. it can be usefull when tring to gain an intial perspective of the dataframe without requiring in depth overview\ndataframe.glimpse(max_items_per_column)\nYou can leave the parameters blank which I would reccomend in most use cases but you can pass it a number to set the max number of items to return for each column.\n\n# Reads data from a parquet file into a Polars DataFrame\n# Parquet is a columnar storage file format optimized for analytics\nfinance_df = pl.read_parquet(\"./data/finance.parquet\")\n\n# Display a summary overview of the DataFrame\nfinance_df.glimpse()\n\nRows: 1310\nColumns: 10\n$ date           &lt;datetime[ns]&gt; 2024-04-15 00:00:00, 2024-04-16 00:00:00, 2024-04-17 00:00:00, 2024-04-18 00:00:00, 2024-04-19 00:00:00, 2024-04-22 00:00:00, 2024-04-23 00:00:00, 2024-04-24 00:00:00, 2024-04-25 00:00:00, 2024-04-26 00:00:00\n$ ticker                  &lt;str&gt; 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL'\n$ open                    &lt;f64&gt; 215.0, 215.27, 216.07, 220.02, 217.83, 216.56, 215.03, 212.29, 203.8, 206.87\n$ high                    &lt;f64&gt; 215.49, 216.26, 219.1, 222.15, 219.19, 218.67, 216.66, 213.96, 205.52, 209.23\n$ low                     &lt;f64&gt; 214.22, 214.32, 215.91, 218.75, 216.63, 215.46, 214.58, 212.31, 202.66, 205.24\n$ close                   &lt;f64&gt; 215.01, 216.14, 217.16, 220.56, 218.74, 217.13, 215.38, 212.5, 204.28, 207.29\n$ volume                  &lt;i64&gt; 1930612, 2166356, 2108441, 2426330, 2922733, 2282139, 2347011, 2170140, 2601900, 2331680\n$ pe_ratio                &lt;f64&gt; 24.63, 11.55, 37.52, 31.55, 31.33, 12.62, 23.63, 14.16, 13.2, 16.15\n$ dividend_yield          &lt;f64&gt; 0.0255, 0.0005, 0.0002, 0.0161, 0.006, 0.0152, 0.0165, 0.0001, 0.0255, 0.0114\n$ market_cap              &lt;f64&gt; 2664107971.0, 5452883768.0, 6751310057.0, 4539928581.0, 5699638189.0, 6553501976.0, 4761567781.0, 2762079236.0, 5364612742.0, 6232280976.0\n\n\n\n\n\n1.4.4 describe\n\nfinance_df.describe()\n\n\nshape: (9, 11)\n\n\n\nstatistic\ndate\nticker\nopen\nhigh\nlow\nclose\nvolume\npe_ratio\ndividend_yield\nmarket_cap\n\n\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n\"1310\"\n\"1310\"\n1310.0\n1310.0\n1310.0\n1310.0\n1310.0\n1310.0\n1310.0\n1310.0\n\n\n\"null_count\"\n\"0\"\n\"0\"\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n\"2024-10-13 21:42:35.725190\"\nnull\n309.888687\n312.208565\n309.056229\n310.665\n3.5360e6\n24.924183\n0.014968\n1.3308e10\n\n\n\"std\"\nnull\nnull\n183.806709\n185.216272\n183.266391\n184.279542\n1.9400e6\n8.695993\n0.008674\n1.5527e10\n\n\n\"min\"\n\"2024-04-15 00:00:00\"\n\"AAPL\"\n125.74\n127.01\n125.19\n125.9\n462641.0\n10.03\n0.0\n1.5396e9\n\n\n\"25%\"\n\"2024-07-15 00:00:00\"\nnull\n200.6\n201.72\n199.93\n200.91\n1.889358e6\n17.27\n0.0075\n3.8667e9\n\n\n\"50%\"\n\"2024-10-15 00:00:00\"\nnull\n240.87\n242.6\n240.33\n241.9\n2.994889e6\n25.02\n0.0152\n5.9477e9\n\n\n\"75%\"\n\"2025-01-14 00:00:00\"\nnull\n315.93\n318.79\n316.15\n316.79\n4.9854e6\n32.54\n0.0223\n1.5756e10\n\n\n\"max\"\n\"2025-04-15 00:00:00\"\n\"MSFT\"\n792.59\n801.45\n790.79\n796.05\n1.0040359e7\n39.99\n0.03\n7.0804e10\n\n\n\n\n\n\n\n\n1.4.5 sample\n\nfinance_df.sample(3)\n\n\nshape: (3, 10)\n\n\n\ndate\nticker\nopen\nhigh\nlow\nclose\nvolume\npe_ratio\ndividend_yield\nmarket_cap\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\n\n\n\n\n2025-01-20 00:00:00\n\"META\"\n710.41\n714.68\n706.4\n713.24\n7272570\n25.07\n0.0275\n5.4399e10\n\n\n2025-01-15 00:00:00\n\"MSFT\"\n212.39\n214.85\n212.94\n213.04\n1129114\n32.02\n0.0102\n3.1547e9\n\n\n2024-10-02 00:00:00\n\"GOOG\"\n263.02\n266.17\n261.31\n263.84\n4272221\n15.77\n0.0215\n1.4172e10\n\n\n\n\n\n\n\n\n1.4.6 slice\ndataframe.slice(offset, length)\n\nfinance_df.slice(500, 6)\n\n\nshape: (6, 10)\n\n\n\ndate\nticker\nopen\nhigh\nlow\nclose\nvolume\npe_ratio\ndividend_yield\nmarket_cap\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\n\n\n\n\n2025-03-13 00:00:00\n\"MSFT\"\n242.34\n244.05\n242.99\n243.12\n1404084\n27.98\n0.0271\n3.2495e9\n\n\n2025-03-14 00:00:00\n\"MSFT\"\n243.49\n244.84\n243.64\n243.76\n462641\n34.61\n0.0248\n4.0134e9\n\n\n2025-03-17 00:00:00\n\"MSFT\"\n247.86\n250.29\n246.92\n248.3\n1637110\n34.78\n0.0189\n4.8262e9\n\n\n2025-03-18 00:00:00\n\"MSFT\"\n242.34\n244.07\n240.73\n243.06\n1112130\n26.23\n0.0182\n2.6502e9\n\n\n2025-03-19 00:00:00\n\"MSFT\"\n243.08\n245.24\n243.39\n243.84\n1346630\n11.79\n0.0181\n4.6632e9\n\n\n2025-03-20 00:00:00\n\"MSFT\"\n243.86\n245.05\n242.87\n244.5\n1202714\n12.4\n0.0133\n2.4545e9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Dataframes and Series</span>"
    ]
  },
  {
    "objectID": "appendix1.html",
    "href": "appendix1.html",
    "title": "Appendix A — Common Data Types",
    "section": "",
    "text": "Data Type\nPolars Type\nDescription\nExample\n\n\n\n\nStrings\npl.Utf8\nText data\n\"hello\"\n\n\nSigned Integers\npl.Int8, pl.Int16, pl.Int32, pl.Int64\nWhole numbers that can be positive or negative\n-42\n\n\nUnsigned Integers\npl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64\nWhole numbers that can only be positive\n42\n\n\nFloating Point\npl.Float32, pl.Float64\nReal numbers with decimal points\n3.14159\n\n\nDecimals\npl.Decimal\nFixed-precision numbers, useful for financial calculations\nDecimal(\"10.99\")\n\n\nDates/DateTimes\npl.Date, pl.Datetime\nCalendar dates and time values\n2023-01-01, 2023-01-01T12:30:00\n\n\nBooleans\npl.Boolean\nLogical values: true or false\nTrue, False\n\n\nTime\npl.Time\nTime of day without date\n12:30:45\n\n\nDuration\npl.Duration\nTime spans or intervals\n3d 12h 30m 45s\n\n\nCategorical\npl.Categorical\nEfficient storage for repeated string values\npl.Series([\"a\", \"b\", \"a\"]).cast(pl.Categorical)\n\n\nList\npl.List\nLists of values of any type\n[1, 2, 3]\n\n\nStruct\npl.Struct\nComposite type with named fields\n{\"field1\": 1, \"field2\": \"a\"}\n\n\nNull\npl.Null\nMissing or undefined values\nNone or null",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Common Data Types</span>"
    ]
  },
  {
    "objectID": "appendix2.html",
    "href": "appendix2.html",
    "title": "Appendix B — Reading and Writing Data",
    "section": "",
    "text": "B.1 CSV Files\nPolars provides robust capabilities for importing data from various sources including CSVs, JSONs, Excel spreadsheets, Parquet files, cloud storage solutions (AWS, Azure, and Google Cloud), and databases.\nThe importing methods follow a consistent pattern across file types, making it easy to work with different data formats.\nThe basic syntax for reading a CSV file is:\nAlternatively, you can also read CSV files directly from the internet:\nThis capability to read files directly from URLs also works with all the file import methods we’ll cover below.\nThis function offers numerous parameters to handle different CSV formats and configurations. For more information read the documentation.\nimport polars as pl\n\ndf_csv = pl.read_csv(\"./data/example.csv\", try_parse_dates=True)\ndf_csv.head(5)\n\n\nshape: (5, 9)\n\n\n\nid\nfirst_name\nlast_name\nemail\npurchase_date\nproduct\nquantity\nprice\ntotal\n\n\ni64\nstr\nstr\nstr\ndate\nstr\ni64\nf64\nf64\n\n\n\n\n1\n\"John\"\n\"Doe\"\n\"john.doe@example.com\"\n2023-01-15\n\"Laptop\"\n1\n1299.99\n1299.99\n\n\n2\n\"Jane\"\n\"Smith\"\n\"jane.smith@example.com\"\n2023-01-16\n\"Smartphone\"\n2\n699.95\n1399.9\n\n\n3\n\"Robert\"\n\"Johnson\"\n\"rob.j@example.com\"\n2023-01-18\n\"Headphones, Wireless\"\n3\n89.99\n269.97\n\n\n4\n\"Sarah\"\n\"Williams\"\n\"sarah.w@example.com\"\n2023-01-20\n\"Monitor\"\n1\n249.5\n249.5\n\n\n5\n\"Michael\"\n\"Brown\"\n\"michael.b@example.com\"\n2023-01-22\n\"Keyboard\"\n2\n59.99\n119.98",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Reading and Writing Data</span>"
    ]
  },
  {
    "objectID": "appendix2.html#csv-files",
    "href": "appendix2.html#csv-files",
    "title": "Appendix B — Reading and Writing Data",
    "section": "",
    "text": "pl.read_csv(\"path/to/data.csv\")\n\npl.read_csv(\"https://example.com/path/to/your/file.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Reading and Writing Data</span>"
    ]
  },
  {
    "objectID": "appendix2.html#json-files",
    "href": "appendix2.html#json-files",
    "title": "Appendix B — Reading and Writing Data",
    "section": "B.2 JSON Files",
    "text": "B.2 JSON Files\nReading JSON files follows a similar pattern. The basic syntax is:\npl.read_json(\"docs/data/path.json\")\nJSON files have a more standardized structure than CSVs, so the reading process requires fewer configuration parameters. Polars handles JSON parsing efficiently with minimal setup. For advanced options and configurations, consult the official documentation.\ndf_json = pl.read_json(\"./data/basketball.json\")\n\ndf_json",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Reading and Writing Data</span>"
    ]
  },
  {
    "objectID": "appendix2.html#excel",
    "href": "appendix2.html#excel",
    "title": "Appendix B — Reading and Writing Data",
    "section": "B.3 Excel",
    "text": "B.3 Excel\nPolars doesn’t include a native Excel reader. Instead, it leverages external libraries like fastexcel, xlsx2csv, or openpyxl to parse Excel files into Polars-compatible formats. Among these options, Polars recommends fastexcel for optimal performance.\nWhile it’s generally better to avoid using Excel files where possible (you can usually export as CSV directly from Excel), reading Excel files is straightforward with the right dependencies installed.\nBefore attempting to read Excel files, make sure you have at least one of these libraries installed:\n $ pip install fastexcel xlsx2csv openpyxl\nThe basic syntax for reading an Excel file with Polars is:\npl.read_excel(\"path/to/data.xlsx\")\nIf your Excel file contains multiple sheets, you can specify which one to read using the sheet_name parameter:\ndf = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\nFor additional Excel reading options and parameters, refer to the Polars Excel documentation, which covers sheet selection, range specification, and handling of complex Excel files.\n\ndf_xlsx = pl.read_excel(\"./data/penguins.xlsx\", sheet_name=\"Dream Island\")\n\ndf_xlsx.tail(5)\n\n\nshape: (5, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\ni64\ni64\nstr\ni64\n\n\n\n\n\"Chinstrap\"\n\"Dream\"\n55.8\n19.8\n207\n4000\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n43.5\n18.1\n202\n3400\n\"female\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n49.6\n18.2\n193\n3775\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.8\n19.0\n210\n4100\n\"male\"\n2009\n\n\n\"Chinstrap\"\n\"Dream\"\n50.2\n18.7\n198\n3775\n\"female\"\n2009\n\n\n\n\n\n\nThis example spreadsheet can be accessed via this Google Sheets link.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Reading and Writing Data</span>"
    ]
  },
  {
    "objectID": "appendix2.html#parquet-files",
    "href": "appendix2.html#parquet-files",
    "title": "Appendix B — Reading and Writing Data",
    "section": "B.4 Parquet Files",
    "text": "B.4 Parquet Files\nParquet is a columnar storage format designed for efficient data analytics. It provides excellent compression and fast query performance, making it a popular choice for data science workflows. Polars includes native, high-performance support for reading Parquet files.\nThe basic syntax for reading a Parquet file is:\npl.read_parquet(\"path/to/data.parquet\")\n\ndf_par = pl.read_parquet(\"./data/finance.parquet\")\ndf_par.sample(4)\n\n\nshape: (4, 10)\n\n\n\ndate\nticker\nopen\nhigh\nlow\nclose\nvolume\npe_ratio\ndividend_yield\nmarket_cap\n\n\ndatetime[ns]\nstr\nf64\nf64\nf64\nf64\ni64\nf64\nf64\nf64\n\n\n\n\n2025-04-03 00:00:00\n\"MSFT\"\n266.11\n267.92\n266.03\n267.08\n1737462\n17.11\n0.0029\n2.7445e9\n\n\n2024-05-15 00:00:00\n\"AAPL\"\n199.12\n199.92\n197.88\n199.71\n2154172\n33.7\n0.0007\n4.8464e9\n\n\n2024-11-19 00:00:00\n\"AAPL\"\n244.37\n246.09\n244.2\n245.1\n1795901\n20.31\n0.0185\n4.8685e9\n\n\n2024-11-18 00:00:00\n\"GOOG\"\n280.24\n281.28\n280.52\n281.05\n5279934\n24.24\n0.0108\n1.1134e10",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Reading and Writing Data</span>"
    ]
  },
  {
    "objectID": "appendix2.html#importing-mutiple-files",
    "href": "appendix2.html#importing-mutiple-files",
    "title": "Appendix B — Reading and Writing Data",
    "section": "B.5 Importing Mutiple files",
    "text": "B.5 Importing Mutiple files\nFor situations where you need to combine data from multiple files into a single DataFrame, Polars offers straightforward approaches. While the syntax is relatively simple, the implementation may vary depending on your specific file organization.\nWhen working with multiple files of the same type and similar naming patterns in a single directory, Polars supports glob pattern matching:\npl.read_filetype(\"path/to/data/my_many_files_*.filetype\")\nFor files with different names but the same format, placing them in a single directory allows you to use wildcard patterns to import them all at once:\npl.read_filetype(\"path/to/data/import/*.filetype\")\nAlternatively, for files located in different directories or even on different servers, you can provide a list of filepaths or URLs:\npl.read_filetype([\n    \"path/to/first/file.filetype\",\n    \"path/to/second/file.filetype\",\n    \"another/location/file.filetype\"\n])\nIf you’re working with different file types that share the same schema (identical columns and datatypes) and want to combine them into a single DataFrame, you’ll need to read each file individually and then concatenate them. Polars makes this process straightforward with its concat function, which can merge DataFrames regardless of their original file formats.\n# Read files of different formats\ndf1 = pl.read_csv(\"path/to/file.csv\")\ndf2 = pl.read_parquet(\"path/to/file.parquet\")\ndf3 = pl.read_json(\"path/to/file.json\")\n\n# Concatenate into a single DataFrame\ncombined_df = pl.concat([df1, df2, df3], how=\"vertical\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Reading and Writing Data</span>"
    ]
  }
]