{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reading and Writing Data {#sec-appendix-b}\n",
        "\n",
        "Polars provides robust capabilities for importing data from various sources including CSVs, JSONs, Excel spreadsheets, Parquet files, cloud storage solutions (AWS, Azure, and Google Cloud), and databases.\n",
        "\n",
        "The importing methods follow a consistent pattern across file types, making it easy to work with different data formats.\n",
        "\n",
        "## CSV Files\n",
        "\n",
        "The basic syntax for reading a CSV file is:\n",
        "```python\n",
        "pl.read_csv(\"path/to/data.csv\")\n",
        "```\n",
        "\n",
        "Alternatively, you can also read CSV files directly from the internet:\n",
        "```python\n",
        "pl.read_csv(\"https://example.com/path/to/your/file.csv\")\n",
        "```\n",
        "This capability to read files directly from URLs also works with all the file import methods we'll cover below.\n",
        "\n",
        "\n",
        "This function offers numerous parameters to handle different CSV formats and configurations. For more information read the [documentation](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html)."
      ],
      "id": "083797a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "\n",
        "df_csv = pl.read_csv(\"./data/example.csv\", try_parse_dates=True)\n",
        "df_csv.head(5)"
      ],
      "id": "ce24a389",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JSON Files\n",
        "\n",
        "Reading JSON files follows a similar pattern. The basic syntax is:\n",
        "```python\n",
        "pl.read_json(\"docs/data/path.json\")\n",
        "```\n",
        "\n",
        "JSON files have a more standardized structure than CSVs, so the reading process requires fewer configuration parameters. Polars handles JSON parsing efficiently with minimal setup. For advanced options and configurations, consult the official [documentation](https://docs.pola.rs/api/python/stable/reference/api/polars.read_json.html).\n",
        "\n",
        "```python\n",
        "df_json = pl.read_json(\"./data/basketball.json\")\n",
        "\n",
        "df_json\n",
        "```\n",
        "\n",
        "## Excel \n",
        "\n",
        "Polars doesn't include a native Excel reader. Instead, it leverages external libraries like fastexcel, xlsx2csv, or openpyxl to parse Excel files into Polars-compatible formats. Among these options, Polars recommends fastexcel for optimal performance. \n",
        "\n",
        "While it's generally better to avoid using Excel files where possible (you can usually export as CSV directly from Excel), reading Excel files is straightforward with the right dependencies installed.\n",
        "\n",
        "Before attempting to read Excel files, make sure you have at least one of these libraries installed:\n",
        "```bash\n",
        " $ pip install fastexcel xlsx2csv openpyxl\n",
        " ```\n",
        "\n",
        "\n",
        "The basic syntax for reading an Excel file with Polars is:\n",
        "```python\n",
        "pl.read_excel(\"path/to/data.xlsx\")\n",
        "```\n",
        "\n",
        "If your Excel file contains multiple sheets, you can specify which one to read using the `sheet_name` parameter:\n",
        "```python\n",
        "df = pl.read_excel(\"path/to/data.xlsx\", sheet_name=\"example\")\n",
        "```\n",
        "\n",
        "For additional Excel reading options and parameters, refer to the [Polars Excel documentation](https://docs.pola.rs/api/python/stable/reference/api/polars.read_excel.html), which covers sheet selection, range specification, and handling of complex Excel files."
      ],
      "id": "59e447eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_xlsx = pl.read_excel(\"./data/penguins.xlsx\", sheet_name=\"Dream Island\")\n",
        "\n",
        "df_xlsx.tail(5)"
      ],
      "id": "85cabc79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example spreadsheet can be accessed via [this Google Sheets link](https://docs.google.com/spreadsheets/d/1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY/edit?gid=0#gid=0).\n",
        "\n",
        "## Parquet Files\n",
        "\n",
        "Parquet is a columnar storage format designed for efficient data analytics. It provides excellent compression and fast query performance, making it a popular choice for data science workflows. Polars includes native, high-performance support for reading Parquet files.\n",
        "\n",
        "The basic syntax for reading a Parquet file is:\n",
        "\n",
        "```python\n",
        "pl.read_parquet(\"path/to/data.parquet\")\n",
        "```"
      ],
      "id": "4f191f70"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_par = pl.read_parquet(\"./data/finance.parquet\")\n",
        "df_par.sample(4)"
      ],
      "id": "44a4b964",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Mutiple files\n",
        "\n",
        "For situations where you need to combine data from multiple files into a single DataFrame, Polars offers straightforward approaches. While the syntax is relatively simple, the implementation may vary depending on your specific file organization.\n",
        "\n",
        "When working with multiple files of the same type and similar naming patterns in a single directory, Polars supports glob pattern matching:\n",
        "\n",
        "```python\n",
        "pl.read_filetype(\"path/to/data/my_many_files_*.filetype\")\n",
        "```\n",
        "For files with different names but the same format, placing them in a single directory allows you to use wildcard patterns to import them all at once:\n",
        "\n",
        "```python\n",
        "pl.read_filetype(\"path/to/data/import/*.filetype\")\n",
        "```\n",
        "Alternatively, for files located in different directories or even on different servers, you can provide a list of filepaths or URLs:\n",
        "\n",
        "```python\n",
        "pl.read_filetype([\n",
        "    \"path/to/first/file.filetype\",\n",
        "    \"path/to/second/file.filetype\",\n",
        "    \"another/location/file.filetype\"\n",
        "])\n",
        "```\n",
        "If you're working with different file types that share the same schema (identical columns and datatypes) and want to combine them into a single DataFrame, you'll need to read each file individually and then concatenate them. Polars makes this process straightforward with its `concat` function, which can merge DataFrames regardless of their original file formats.\n",
        "\n",
        "```python\n",
        "# Read files of different formats\n",
        "df1 = pl.read_csv(\"path/to/file.csv\")\n",
        "df2 = pl.read_parquet(\"path/to/file.parquet\")\n",
        "df3 = pl.read_json(\"path/to/file.json\")\n",
        "\n",
        "# Concatenate into a single DataFrame\n",
        "combined_df = pl.concat([df1, df2, df3], how=\"vertical\")\n",
        "```"
      ],
      "id": "61e204af"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/nathaniel/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}