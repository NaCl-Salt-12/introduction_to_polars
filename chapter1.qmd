---
title: Dataframes and Series
format: live-html
---

## Data types

Polars allows you to store data in a variety of formats called data types. These data types fall generally into the following categories:

- **Numeric**: Signed integers, unsigned integers, floating point numbers, and decimals
- **Nested**: Lists, structs, and arrays for handling complex data
- **Temporal**: Dates, datetimes, and times for working with time-based data
- **Miscellaneous**: Strings, binary data, Booleans, categoricals, enums, and objects

The most common data types you will be working with are generally: Strings, signed and unsigned integers, floating point numbers or floats, decimals, dates or datetimes and booleans. For more information on each of these data types see [Appendix A](appendix1.qmd).

## Series

The two most common data structures in Polars are DataFrames and Series. Series are one-dimensional data structures where all elements share the same datatype. Think of a Series as a single column in a table - it's essentially a named array of data. If a DataFrame is like a spreadsheet, a Series is like a single column in Excel that you've given a name.

Creating a Series is straightforward with the following syntax:

`pl.Series(name, values_list)`

Where "name" is the label for your Series and "values_list" contains the data. Here's a simple example:
```{python}
import polars as pl
s = pl.Series("example", [1, 2, 3, 4, 5])
s
```

When you create a series Polars will infer the data type for the values you provide. So in the above example I gave it [1, 2, 3, 4, 5] and it set the datatype to Int64 if instead gave it [1, 2, 3, 4.0, 5] it would asume it is Float64.


```{python}
# When you create a series Polars will infer the data type
# Much like how my roomate "infers" that 3 AM is the perfect time to get a snack
s2 = pl.Series("payment", [132.50, 120, 116, 98.75, 42])
s2
```

```{python}
s3 = pl.Series("mixed", [1, "text", True, 3.14], strict=False)
# series.dytpe outputs a the data type of the series
print(f"Mixed series type: {s3.dtype}")
s3
```

You can set the data type of the series as well by using the `dtype` parameter. A example use case is when storing a id number the id number should be stored as a string not a int due to the fact that we we do not want to perform mathmatical operations on the identification number therefore it is best stored as a string.


```{python}
# strict=False allows automatic conversion from different data types
s3 = pl.Series("id number", [143823, 194203, 553420, 234325, 236532], dtype=pl.Utf8, strict=False)
s3
```

## Dataframes

DataFrames are tabular data structures (rows and columns) composed of multiple Series, with each column representing a single Series. The design of a dataframe is called schema. A schema is a mapping of column to the data types. 

Dataframes are the workhorses of data analysis and what you’ll use most frequently.

With DataFrames, you can write powerful queries to filter, transform, aggregate, and reshape your data efficiently.

DataFrames can be created in several ways:

1. From a dictionary of sequences (lists, arrays)
2. With explicit schema specification 
3. From a sequence of (name, dtype) pairs
4. From NumPy arrays
5. From a list of lists (row-oriented data)
6. By converting pandas DataFrames
7. By importing existing tabular data from CSVs, JSON, SQL, Parquet files, etc.

In real-world environments, you’ll typically work with preexisting data, though understanding various creation methods is valuable. We’ll cover data import techniques later, but for now, here’s an example of a DataFrame created from a dictionary of lists:
```{python}
# Create a DataFrame from a dictionary of lists
df = pl.DataFrame({
    "name": ["Alice", "Bob", "Charlie", "David"],
    "age": [25, 30, 35, 40],
    "city": ["New York", "Boston", "Chicago", "Seattle"],
    "salary": [75000, 85000, 90000, 95000]
})

df
```

every data frame has a shape. the shape is the number of rows and columns in a dataframe 
`shape(rows,columns)`

the shape for the above dataframe is:

```{python}
print(df.shape)
```

you can view the schema of any dataframe with the following command 

```{python}
print(df.schema)
```

We see here that the schema is returned as a dictionary. In the above example the column name has the string datatype. Though you can view the data type already when displaying the dataframe.

## Inspecting Dataframes 

In polars there are a varity of ways to inspect a dataframe, all of which have different use cases. The ones that we will be covering right now are:


:::{.panel-tabset}
### head 

the `head` functions allows you to view the first x rows of the dataframe. By default the number of rows it shows is 5, though you can specify the number of rows to view.

```python
dataframe.head(n)
```
Where n is the number of rows to return if you give it a negative number it will turn all rows except the last n rows.

```{python}
import numpy as np

# Create NumPy arrays for sandwich data
# anyone else feeling hungry
sandwich_names = np.array(['BLT', 'Club', 'Tuna', 'Ham & Cheese', 'Veggie'])
prices = np.array([8.99, 10.50, 7.50, 6.99, 6.50])
calories = np.array([550, 720, 480, 520, 320])
vegetarian = np.array([False, False, False, False, True])

# Create DataFrame from NumPy arrays
sandwich_df = pl.DataFrame({
    "sandwich": sandwich_names,
    "price": prices,
    "calories": calories,
    "vegetarian": vegetarian
})


sandwich_df.head(3)
```

::: {.callout-tip}
Both head and tail are useful for quick data exploration without loading the entire dataset.
:::

### tail

The `tail` function is essentially the inverse of head. It allows you to view the last n rows of the dataframe. The default for tail is also five rows.

```python
dataframe.tail(n)
```
Where n is the number of rows to return if you give it a negative number it will turn all rows except the first n rows.


```{python}
# Import data from the sales.csv file into a Polars DataFrame
sales_df = pl.read_csv("./data/sales.csv")

# Display the last 6 rows of the sales DataFrame
sales_df.tail(6)
```

::: {.callout-note}
For more information on reading in external data see [Appendix B](appendix2.qmd#sec-appendix-b).
:::

### glimpse

The `glimpse` function allows you to preview your dataframe. By providing the number of rows and columns. The column names and datatypes and the first few values of each column. it can be usefull when tring to gain an intial perspective of the dataframe without requiring in depth overview

```python
dataframe.glimpse(max_items_per_column)
```
You can leave the parameters blank which I would reccomend in most use cases but you can pass it a number to set the max number of items to return for each column. This can be useful when dealing with a large number of columns.

```{python}
# Reads data from a parquet file into a Polars DataFrame
# Parquet is a columnar storage file format optimized for analytics
finance_df = pl.read_parquet("./data/finance.parquet")

# The glimpse function: Think of it as speed dating with your data
# You don't get to know everything, but you'll know if you want a second date
finance_df.glimpse()
```


### describe

The `describe`function provides summary statistics for each column, including count, the count of missing values, mean, standard deviation, mininum, maxiumum and the four quartiles.

```{python}
finance_df.describe()
```


::: {.callout-note}
This will also attempt to find min and max for string values which will be alphabetically.
:::

### sample



```{python}
finance_df.sample(3)
```

### slice

```python
dataframe.slice(offset, length)
```

```{python}
finance_df.slice(500, 6)
```


## Summary

## Practice
